# Deep Belief Networks
*[Hinton06]* cho thấy RBMs có thể xếp chồng lên nhau và được đào tạo một cách tham lam để tạo thành cái gọi là Deep Belief Networks(DBN). DBNs là graphical models học cách trích xuất một đại diện phân cấp sâu của training data. They model phân bố chung giữa observed vector x and the l hidden layers h^k như sau:</br>
![](https://latex.codecogs.com/gif.latex?P%28x%2C%20h%5E1%2C%20%5Cldots%2C%20h%5E%7B%5Cell%7D%29%20%3D%20%5Cleft%28%5Cprod_%7Bk%3D0%7D%5E%7B%5Cell-2%7D%20P%28h%5Ek%7Ch%5E%7Bk&plus;1%7D%29%5Cright%29%20P%28h%5E%7B%5Cell-1%7D%2Ch%5E%7B%5Cell%7D%29)
</br>
where ![](https://latex.codecogs.com/gif.latex?x%3Dh%5E0%2C%20P%28h%5E%7Bk-1%7D%20%7C%20h%5Ek%29) là phân phối có điều kiện chó các đợi vị hiển thị được điều chỉnh trên hidden units của RBM tại level k, and ![](https://latex.codecogs.com/gif.latex?P%28h%5E%7B%5Cell-1%7D%2C%20h%5E%7B%5Cell%7D%29) là phân phối chung có thể nhìn thấy được ẩn trong top-level RBM. Điều này được minh họa trong hình bên dưới.</br>
![](https://github.com/bigkizd/Deep_Learning/blob/master/Deep%20Belief%20Networks/Images/DBN3.png)</br>
Nguyên tắc đào tạo không được giám sát theo tham lam có thể được áp dụng cho các DBNs với RBMs như các khối xây dựng cho mỗi lớp*[Hinton06], [Bengio07]*. Qúa trình này như sau:</br>
1. Train the first layer như RBM that models the raw input ![](https://latex.codecogs.com/gif.latex?x%20%3D%20h%5E%7B%280%29%7D) as its visibel layer.
2. Sử dụng lớp đầu tiên đó để có được một biểu diễn đầu vào sẽ được sử dụng làm sạch dữ liệu cho lớp thứ 2. Two common solutions exist. Biếu diễn này có thể được chọn làm mean activations ![](https://latex.codecogs.com/gif.latex?p%28h%5E%7B%281%29%7D%3D1%7Ch%5E%7B%280%29%7D%29) or samples của ![](https://latex.codecogs.com/gif.latex?p%28h%5E%7B%281%29%7D%7Ch%5E%7B%280%29%7D%29).
3. Train the second layer dưới dạng RBM, lấy dữ liệu được chuyển đổi(samples or mean activations) as training examples(for the visible layer ò that RBM).
4. Lặp lại(2 and 3) cho số layers mong muốn, mỗi lần truyền lên hoặc là samples hoặc là mean values.
5. Tinh chỉnh tất cả các thông số của kến trúc sâu này đối với proxy for the DBN log-likehood, or liên quan đến một tiêu chí đào tạo được giám sát(after adding extra learning machinery để chuyển đổi biểu diễn đã học thành các dự đoán được giám sát, e.g. a linear classifier.</br>
</br>
Trong hướng dẫn này, chúng tôi tập trung vào tinh chỉnh thông qua supervised gradient descent. Cụ thể, chúng tôi sử dụng a logistic regression classifier để phân loại đầu vào x  dựa trên đầu ra của lớp ẩn cuối cùng của DBN. Tinh chỉnh sau đó được thực hiện thông qua supervised descent của negative log-likelihood cost function. Vì the supervised gradient chỉ không có giá trị cho trọng số và hiden layer biases of each layer(i.e. null for the visible biases of each RBM), this proceduce tương đương với việc khởi tạo các tham số của a deep MLP với trọng số và hidden layer biases thu được với chiến lược huấn luyện không giám sát.
# Justifyting Greedy-Layer Wire Pre-Training
why does such an algorithm work? Lấy ví dụ a 2-layer DBN with hidden layers ![](https://latex.codecogs.com/gif.latex?h%5E%7B%281%29%7D) and  ![](https://latex.codecogs.com/gif.latex?h%5E%7B%282%29%7D) (với tham số trọng số tương ứng ![](https://latex.codecogs.com/gif.latex?W%5E%7B%281%29%7D) and ![](https://latex.codecogs.com/gif.latex?W%5E%7B%282%29%7D)
